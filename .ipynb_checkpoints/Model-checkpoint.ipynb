{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "# from skimage import io, transform\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "# import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "train_features = ['./RawTrainingFeatures1.csv','./RawTrainingFeatures2.csv']\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 64 output channels, 1x6  convolution\n",
    "        # kernel\n",
    "        self.local_conv = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(1,6), padding=(0, 2))\n",
    "        \n",
    "        # 64 input channels (check this?), feature maps from local convolution,\n",
    "        # 128 output channels, 20x2 kernel (check this?)\n",
    "        self.global_conv=nn.Conv2d(64,128,(88,2))\n",
    "        \n",
    "        #LSTM layer,48 cells each\n",
    "        self.dec=nn.LSTM(128,48,2,dropout=0.25)\n",
    "        \n",
    "        # Size of output of LSTM, for now use # of hiden state features\n",
    "        self.denseFF=nn.Linear(48,7)\n",
    "        self.sm=nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply ReLu units to the results of convolution, local convoltion layer\n",
    "        x=x.float()\n",
    "        x=F.relu(self.local_conv(x))\n",
    "        x = nn.ZeroPad2d((0,1,0,0))(x)\n",
    "        x=nn.MaxPool2d(kernel_size=(1,4))(x)\n",
    "        #Global convolution layer\n",
    "        x=F.relu(self.global_conv(x))\n",
    "        x = nn.ZeroPad2d((0,1,0,0))(x)\n",
    "        x=nn.MaxPool2d(kernel_size=(1,2))(x)\n",
    "        # remove second dimension\n",
    "        # x=torch.squeeze(input=x, dim=0)\n",
    "        x = x.permute(3, 0, 2, 1)\n",
    "        x = torch.squeeze(x, dim=2)\n",
    "        out,hidden=self.dec(x)\n",
    "        # Feed output through dense dense/feedforward layer with softmax activation units to\n",
    "        # classify the input onto one of the 7 emotion categories.\n",
    "        out = out[-1, :, :]\n",
    "        out=self.sm(self.denseFF(out))\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csvs, transform=None,test=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        csv_file_path_1=csvs[0]\n",
    "        csv_file_path_2=csvs[1]\n",
    "        emotions_frame1 = pd.read_csv(csv_file_path_1,header=None)\n",
    "        emotions_frame2 = pd.read_csv(csv_file_path_2,header=None)\n",
    "#         self.emotions_frame=pd.concat([emotions_frame1,emotions_frame2])\n",
    "        self.emotions_frame=emotions_frame1\n",
    "        if not(test):\n",
    "            self.emotions_frame,self.test_frames=train_test_split(self.emotions_frame,test_size=0.1)\n",
    "        else:\n",
    "            self.test_frames=None\n",
    "        \n",
    "        num_speakers = 250 ##self.emotions_frame.shape[0]\n",
    "        self.transform = transform\n",
    "        self.speaker_map={}\n",
    "        features = self.emotions_frame.iloc[:, 1:-1].as_matrix()\n",
    "        labels =self.emotions_frame.iloc[:,-1]\n",
    "        speakers=self.emotions_frame.iloc[:,0]\n",
    "        speaker_array = [\"\"]*num_speakers\n",
    "       \n",
    "        j=0 \n",
    "        num_features = len(features)\n",
    "\n",
    "        data_array=np.zeros((num_speakers,88,512),dtype='double')\n",
    "        label2index = {\n",
    "        \"anger\":0,\n",
    "        \"boredom\":1,\n",
    "        \"disgust\":2,\n",
    "        \"fear\":3,\n",
    "        \"happiness\":4,\n",
    "        \"sadness\":5,\n",
    "        \"neutral\":6\n",
    "        }\n",
    "        \n",
    "        label_array=['']*num_speakers\n",
    "        for i in range(num_speakers):\n",
    "            initialID= speakers[j]\n",
    "            speaker=initialID[1:3]\n",
    "            speaker_array[i] = initialID\n",
    "            temp_array= features[j, :]\n",
    "            temp_array=np.reshape(temp_array,(88,1))\n",
    "            j+=1\n",
    "#             print(labels[j])\n",
    "#             print(j,num_features,speakers[j],initialID)\n",
    "            idx = label2index[labels[j]]\n",
    "            label_array[i]= idx\n",
    "           \n",
    "            while j < num_features and speakers[j]==initialID:\n",
    "                temp_array = np.hstack((temp_array,np.reshape(features[j, :],(88,1))))\n",
    "                j+=1\n",
    "            if temp_array.shape[1]<512:\n",
    "                pad_length = 512-temp_array.shape[1]\n",
    "                temp_array = np.pad(temp_array,((0, 0), (0, pad_length)),'constant')\n",
    "            elif temp_array.shape[1]>512:\n",
    "                temp_array=temp_array[:,:512]\n",
    "            data_array[i]=temp_array\n",
    "            \n",
    "            if speaker in self.speaker_map:\n",
    "                self.speaker_map[speaker]=np.append(self.speaker_map[speaker],temp_array[newaxis,::],axis=0)\n",
    "            else:\n",
    "                self.speaker_map[speaker]=np.empty((1,88,512))\n",
    "                self.speaker_map[speaker][0,:,:]=temp_array\n",
    "        self.features = data_array\n",
    "        self.labels = label_array\n",
    "        self.speakers = speaker_array\n",
    "        self.std_map={}\n",
    "        self.mean_map={}\n",
    "        for ID in self.speaker_map.keys():\n",
    "            std=np.std(self.speaker_map[ID],axis=(0,2))\n",
    "            std=std[:,newaxis]\n",
    "            std_zeros= std==0\n",
    "            std[std_zeros]=1\n",
    "            \n",
    "            mean=np.mean(self.speaker_map[ID],axis=(0,2))\n",
    "            mean=mean[:,newaxis]\n",
    "            for j in range(511):\n",
    "                std=np.insert(std,1,std[:,0],axis=1)\n",
    "                mean=np.insert(mean,1,mean[:,0],axis=1)\n",
    "            self.std_map[ID]=std\n",
    "            self.mean_map[ID]=mean\n",
    "    def get_test_csv(self):\n",
    "        return self.test_frame.to_csv()\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        speaker = self.speakers[idx]\n",
    "        \n",
    "        features = self.features[idx, :, :].astype(\"double\")\n",
    "        features=(features-self.mean_map[speaker[1:3]])/self.std_map[speaker[1:3]]\n",
    "        features = transforms.ToTensor()(features)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        sample = {'speaker': speaker, 'label': label,'features':features}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to train \n",
    "data=EmotionDataset(train_features)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=data, batch_size=64, shuffle=False)\n",
    "model=Net()\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "running_loss = 0\n",
    "for epoch in range(10):\n",
    "    for sample in data_loader:\n",
    "        features = sample[\"features\"]\n",
    "        label = torch.tensor(sample[\"label\"])\n",
    "        y_pred = model(features)\n",
    "        loss=loss_fn(y_pred,label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
